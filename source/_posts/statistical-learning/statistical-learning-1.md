---
title: 不确定性与信息
date: 2024-07-29 23:27:40
tags: 
    - statistical learning
    - notes
categories: 
    - statistical learning
excerpt: 本系列为清华大学《因果与统计学习》课程笔记
---
# Statistical Learning
统计学习算法是将数据经过特定运算得到一个特定结果的算法。那么我们是否应该相信统计学习算法得到的结果？本课程的目的是从统计学角度分析统计学习到底发生了什么，以及找到一个合理的对统计学习及其结果的解释。


# Develop a Maxwell's Demon
## Maxwell's Demon
麦克斯韦妖（Maxwell's demon），是在物理学中假想的妖，能探测并控制单个分子的运动，于1871年由英国物理学家詹姆斯·麦克斯韦为了说明违反热力学第二定律的可能性而设想的。虽然后续的物理学发展表明，在这个过程中实际上发生了能量耗散，但我们仍可以采用该说法。我们定义：麦克斯韦妖是消耗能量，从一个数据中获取信息的实体。

然而，这个说法实际上隐含了两个假设：其一是数据真的包含信息，其二是这个实体能够正确的获取信息。故我们将探究以下这两个问题：
1. 数据包含信息吗？
2. 我们能够从数据中获取正确的信息吗？

# Information in the data
## What is information? From Maxwell to Shannon and then Landau

### The essence of information
#### Maxwell's Demon and the essence of information
当前对信息的定义是说信息是消除不确定性的输入。这个定义的表述经过了多次演变。

- 热力学观点
    麦克斯韦在1867年给出了如下的思想实验：
    在一个封闭的容器中，分子随机运动。假设有一个小妖精（麦克斯韦妖）站在容器中间的隔板上，它可以打开和关闭隔板上的小门。当分子从左侧以较低速度运动时，妖精会打开小门让它们通过；当分子从右侧以较高速度运动时，妖精会关闭小门。这样，左侧的分子速度会逐渐降低，而右侧的分子速度会逐渐升高，从而导致温度差异的产生。

    考虑热力学第二定律（克劳修斯, 1854）：若没有外部能量注入，热量无法从低温传递到高温。而这个思想实验中，妖精可以通过控制分子运动来实现热量从低温到高温的传递，若无外部能量注入，该过程违反热力学第二定律，于是证明“信息若免费则破坏物理定律”。这给出了一个暗示，即信息可能替代能量。信息的获取与操作必须消耗能量，否则会导致物理定律的破坏。

- 物理本质
    齐拉德·莱奥（Szilárd Leó, 1929）提出了一个更为精确的模型来描述麦克斯韦妖的工作原理。他设想妖精需要测量分子的速度，并根据测量结果打开或关闭小门。这个测量过程需要消耗能量，产生熵增抵消系统熵减，因此妖精不能无限制地获取信息而不付出代价。

    后续物理学家提出了“信息的就是物理的”（Information is Physical）的观点，强调信息是定义在系统物理状态上的物理量，信息-能量-质量是等价的。这在物理上给出了信息的本质。

- 信息论定义
    香农（Claude Shannon, 1948）在其论文《通信的数学理论》中提出了信息论，定义了信息的量化方法。香农的信息量度是基于概率的，定义为信息的不确定性减少量。

#### Learning model is a type of Maxwell demon
机器学习模型通过决策边界对数据空间进行划分，其本质与麦克斯韦妖操控分子运动高度相似：

- 麦克斯韦妖的操作：
感知分子速度 → 控制无摩擦门 → 将低速分子隔离于A区，高速分子集中于B区 → 打破系统平衡（熵减）。

- 机器学习模型的操作：
学习数据特征 → 生成决策边界 → 将数据划分为不同区域 → 打破数据混沌（信息熵减）。

二者均通过分类行为将无序系统转化为有序状态，且该过程依赖信息的生成与利用。以贝叶斯最优分类器为例。分类前，数据点处于混沌状态：当我们随机抽样一个点时，其标签归属具有高度不确定性，因为两类点交错分布且无明确区分依据。分类后，机器学习模型通过注入新信息改变系统状态。它生成最优决策边界，将系统划分为两个明确区域，使每个数据点获得"所属分区"的新特征属性。当再次抽样时，我们可基于该点的分区信息显著提升标签判别的确定性：若点位于iaoq2高高高概率区，则其为这种标签的置信度大幅增加。这一过程本质是通过穷尽信息生成决策边界（因生成密度已知而可精确计算），从而降低系统不确定性。

### Quantifying information
如果机器学习模型生产了新的信息从而显著减少了系统的不确定性，那么机器学习模型就是有价值的。于是我们需要寻找到一种量化信息的方法从而评估机器学习模型的价值。

#### Preparation: model the uncertainty
由于我们通过测量系统不确定性的变化来衡量信息，我们需要对“不确定性”进行数学上的建模。数学上使用随机变量来建模不确定性。

- 随机变量
    设 $(\Omega, \mathcal{F}, P)$ 为概率空间，其中：  
    - $\Omega$ 为样本空间（所有可能结果的集合）  
    - $\mathcal{F}$ 为 $\Omega$ 上的 $\sigma$-代数（事件域）  
    - $P$ 为定义在 $\mathcal{F}$ 上的概率测度  

    一个随机变量 $X$ 是从样本空间 $\Omega$ 到实数集 $\mathbb{R}$ 的可测函数，即满足：  
    $$
    \forall B \in \mathcal{B}(\mathbb{R}),\quad X^{-1}(B) = \{ \omega \in \Omega : X(\omega) \in B \} \in \mathcal{F}
    $$  
    其中 $\mathcal{B}(\mathbb{R})$ 是 $\mathbb{R}$ 上的 Borel $\sigma$-代数。  

- 随机变量的特征
  - 概率分布族（Family of Distributions）
- 核心概念：  
  随机变量的概率行为由分布族 $\mathcal{P} = \{ P_\theta : \theta \in \Theta \}$ 描述，其中：  
  - $\Theta$ 为参数空间  
  - $P_\theta$ 是由参数 $\theta$ 确定的概率测度  

- 矩（Moments）
    随机变量的 $n$ 阶矩刻画其分布形态：  
    - $n$ 阶原点矩：  
    $$
    \mu_n = \mathbb{E}[X^n] = \int_\mathbb{R} x^n \, dF_X(x)
    $$  
    其中 $F_X(x)$ 是 $X$ 的累积分布函数（CDF）。  
    - $n$ 阶中心矩：  
    $$
    m_n = \mathbb{E}\left[(X - \mu)^n\right], \quad \mu = \mathbb{E}[X]
    $$  

    | 阶数 $n$ | 矩类型       | 物理意义                  | 分布形态描述              |  
    |-----------|-------------|-------------------------|-------------------------|  
    | 1         | 一阶原点矩    | 期望 $\mu$             | 分布的中心位置            |  
    | 2         | 二阶中心矩    | 方差 $\sigma^2$        | 离散程度（数据稀疏性）     |  
    | 3         | 三阶标准矩    | 偏度 $\gamma_1 = \frac{m_3}{\sigma^3}$ | 分布不对称性（右偏>0，左偏<0） |  
    | 4         | 四阶标准矩    | 峰度 $\gamma_2 = \frac{m_4}{\sigma^4} - 3$ | 尾部厚重性（高峰度→重尾） |  


#### Develop the metric quantifying information

- 基础：二元不确定事件
    考虑一个黑盒系统：  
    - 盒内小球位置由随机变量 $x$ 描述：  
    - $x = 0$：小球位于左侧  
    - $x = 1$：小球位于右侧  
    - 初始状态：  
    $x$ 有 2 种可能取值 → 不确定性为 1 个"二元等价盒子"（bit）  
    - 信息注入后：  
    若盒子左侧变为透明（直接观测到小球位置），则：  
    - $x$ 的可能取值从 2 降为 1（例如确认 $x=0$)  
    - 不确定性完全消除（$\log_2 2 - \log_2 1 = 1$ bit）  

    核心观察：信息通过减少可能状态数消除不确定性，减少量可量化为等价二元盒子数量。  


- 扩展：多元二元事件 
    设系统包含 $m$ 个独立二元事件（如 3 个黑盒各藏一球）：  
    - 联合随机变量：$\mathbf{x} = (x_1, x_2, \dots, x_m)$，其中 $x_i \in \{0,1\}$  
    - 可能状态总数：$K = 2^m$（例如 $m=3$ 时 $K=8$)  
    - 信息的影响：  

    | 信息获取程度          | 剩余可能状态数 $K_{\text{剩余}}$ | 消除的不确定性（bit） |  
    |----------------------|-----------------------------------|---------------------|  
    | 无信息（$\mathbf{x}$ 完全未知） | $2^m$                         | 0                   |  
    | 已知 $x_1 = 1$     | $2^{m-1}$                     | $\log_2 2^m - \log_2 2^{m-1} = 1$ |  
    | 已知 $x_1=1, x_2=0$ | $2^{m-2}$                     | $\log_2 2^m - \log_2 2^{m-2} = 2$ |  

    结论：每消除一个二元事件的不确定性 → 减少 $\log_2 2 = 1$ bit 不确定性 → 等价于消除 1 个二元盒子的熵。  


- 信息度量的通用公式  
    对任意具有 $K_1$ 种可能状态的系统：  
    - 初始不确定性：$U_{\text{初始}} = \log_2 K_1$（bit）  
    - 注入信息后：状态数缩减至 $K_2$  
    - 信息量：  
    $$
    I = \Delta U = \log_2 K_1 - \log_2 K_2 = \log_2 \left( \frac{K_1}{K_2} \right)  \quad \text{(bit)}
    $$  
    直观理解：$I$ 表示信息所消除的等价二元盒子数量（例如 $K_1=8, K_2=4$ 时 $I=1$ bit）。  


- 香农信息度量的公理化定义  
    基于上述思想，Shannon (1948) 建立信息度量体系：  
    1. 基本单位：1 bit = 消除 1 个二元盒子不确定性的信息量  
    2. 广义解释：  
       - 系统状态空间大小从 $K_1$ 减至 $K_2$ → 信息量 $I = \log_2 (K_1 / K_2)$  
       - 状态数缩减比例 $K_1/K_2$ 越大 → 信息量越大  
#### Shannon Entropy: how much information is needed to fully erase the uncertainty of a random variable 

- 随机变量的等价二元盒子量化
  - 核心公式：  
    对于取值空间为 $X = \{x_1, x_2, \dots, x_m\}$ 的随机变量 $x$，其每个取值 $x_i$ 的不确定性等价于：  
    $$
    I_i = \log_2 \frac{1}{P(x_i)} \quad \text{(bit)}
    $$  
    物理意义：  
    - 若事件 $\{x = x_i\}$ 的概率为 $P(x_i)$，则其不确定性相当于 $\log_2 \frac{1}{P(x_i)}$ 个二元盒子（bit）。  
    - 推导逻辑：概率 $\lambda = P(x_i)$ 隐含 $\frac{1}{\lambda}$ 种等可能情况 → 等价于 $\log_2 \frac{1}{\lambda}$ 个二元盒子。  

  - 整体不确定性度量（香农熵）：  
    $$
    H(x) = \sum_{i=1}^m P(x_i) \cdot \log_2 \frac{1}{P(x_i)} = -\sum_{i=1}^m P(x_i) \log_2 P(x_i)
    $$  
    解释：  
    - $H(x)$ 是消除 $x$ 全部不确定性所需信息的期望量（单位：bit）。  
    - 本质是信息量随机变量 $I = \log_2 \frac{1}{P(x)}$ 的期望值：$H(x) = \mathbb{E}[I]$。  

- 香农熵的性质与示例
- 极值性质：  
  | 概率分布          | 香农熵 $H(x)$ | 物理意义               |  
  |-------------------|------------------|----------------------|  
  | 确定性分布（$P(x_k)=1$） | 0 bit           | 无不确定性             |  
  | 均匀分布（$P(x_i)=\frac{1}{m}$) | $\log_2 m$ bit | 不确定性最大           |  

- 黑白球抽样实验：  
  - 系统：黑盒中有 $X$ 个黑球和 $Y$ 个白球（$M = X + Y$)，抽样颜色由 $x$ 描述：  
    $$
    P(x=0) = \frac{X}{M}, \quad P(x=1) = \frac{Y}{M}
    $$  
  - 熵计算：  
    $$
    H(x) = -\left[ \frac{X}{M} \log_2 \frac{X}{M} + \frac{Y}{M} \log_2 \frac{Y}{M} \right]
    $$  
  - 最大熵条件：当 $X = Y$ 时 $H(x)$ 最大（系统最混乱）。  

- 在连续随机变量上的推广
    离散随机变量的香农熵框架完美解释了有限状态系统的不确定性消除机制。然而，当面对连续随机变量（如温度、电压等连续量）时，其取值空间无限且不可数，直接应用离散熵公式将导致发散。为解决此问题，需通过离散化逼近自然导出连续熵的定义：  
    将连续变量 $X$ 的定义域分割为宽度为 $\Delta$ 的区间 $\{B_i\}_{i=1}^n$，例如：  
    $$
    B_i = [x_0 + (i-1)\Delta, x_0 + i\Delta)
    $$  
    构造离散随机变量：  
    定义离散随机变量 $X^\Delta$ 表示 $X$ 所属的区间：  
    $$
    P(X^\Delta = i) = \int_{B_i} f(x)  dx \approx f(x_i) \Delta \quad (x_i \in B_i)
    $$  
    计算离散熵：  
    $$
    H(X^\Delta) = -\sum_i P(X^\Delta = i) \log_2 P(X^\Delta = i) \approx -\sum_i f(x_i) \Delta \log_2 (f(x_i) \Delta)
    $$  
    展开得：  
    $$
    H(X^\Delta) = \underbrace{-\sum_i f(x_i) \Delta \log_2 f(x_i)}_{\text{收敛项}} \underbrace{-\log_2 \Delta \sum_i f(x_i) \Delta}_{-\log_2 \Delta \cdot 1}
    $$  

    当 $\Delta \to 0$ 时，第二项 $-\log_2 \Delta \to +\infty$（因 $\Delta \to 0^+$），第一项收敛至积分 $-\int f(x) \log_2 f(x)  dx$。此极限过程自然导出连续熵的核心定义：  

- 连续随机变量的信息度量：微分熵与互信息 
    - 微分熵（Differential Entropy）：  
    $$
    h(X) = -\int_{-\infty}^{\infty} f(x) \log_2 f(x)  dx
    $$  
    极值：

    | 约束条件         | 最大微分熵分布         | 微分熵表达式                     |  
    |------------------|------------------------|--------------------------------|  
    | 定义域 $\mathbb{R}$ | 高斯分布 $\mathcal{N}(\mu, \sigma^2)$ | $h(X) = \frac{1}{2} \log_2 (2\pi e \sigma^2)$ |  
    | 定义域 $[a,b]$   | 均匀分布 $\mathcal{U}[a,b]$ | $h(X) = \log_2 (b-a)$       |  

    - 互信息（Mutual Information）：  
        $$
        I(X;Y) = \iint f_{X,Y}(x,y) \log_2 \frac{f_{X,Y}(x,y)}{f_X(x) f_Y(y)}  dx dy
        $$  
        - 直观理解：观测 $Y$ 后关于 $X$ 不确定性的减少量  
        - 优势：  
            - 对连续变量恒非负  
            - 满足 $I(X;Y) = h(X) + h(Y) - h(X,Y)$（类比离散形式）  



### Why must we quantify the information
#### 信息比较
考虑黑盒中有 2 个球（可能为黑或白），随机抽取一球：  
- 初始状态（无信息）：  
  - 需建立联合随机变量 $\mathbf{c} = (c_1, c_2)$ 描述两球颜色  
  - 可能状态：4 种（黑黑/黑白/白黑/白白）  
  - 抽样球颜色 $x$ 的熵：  
    $$
    H_{\text{初始}}(x) = \log_2 4 = 2 \ \text{bit} \quad (\text{均匀分布假设})
    $$  

- 信息类型比较：  
  | 信息类型               | 信息注入后系统状态                  | $x$ 的剩余熵 $H_{\text{新}}(x)$ | 信息量 $I = H_{\text{初始}} - H_{\text{新}}$ |  
  |------------------------|-----------------------------------|----------------------------------------|-------------------------------------------------|  
  | 告知球的颜色分布   | 例如 $P(\text{黑}) = 0.8$      | $-[0.8\log_2 0.8 + 0.2\log_2 0.2] \approx 0.72$ bit | $2 - 0.72 = 1.28$ bit |  
  | 告知某一球的颜色   | 例如 $c_1 = \text{黑}$          | $\log_2 2 = 1$ bit（剩余球等概率） | $2 - 1 = 1$ bit |  

结论：  
- 当颜色分布高度偏斜（如 $P(\text{黑})=0.99$) 时，分布信息量更大（$I \approx 1.92$ bit）  
- 香农熵量化了不同信息的价值，证明分布信息通常比单一观测信息更有效  


#### 机器学习中的信息量化
场景：淘宝用户行为预测  
- 初始状态：  
  用户对 $n$ 类商品（如医药/饮料/家居/食品等）的购买概率均匀分布：  
  $$
  P_{\text{初始}}(X=i) = \frac{1}{n} \quad \Rightarrow \quad H_{\text{初始}}(X) = \log_2 n  
  $$  

- ML 模型注入信息后：  
  输出购买概率分布 $P_{\text{ML}}(X=i)$ → 新熵值：  
  $$
  H_{\text{ML}}(X) = -\sum_{i=1}^n P_{\text{ML}}(i) \log_2 P_{\text{ML}}(i)  
  $$  

- 信息量度量：  
  $$
  I_{\text{ML}} = H_{\text{初始}}(X) - H_{\text{ML}}(X)
  $$  
  物理意义：ML 模型通过消费数据生成 $I_{\text{ML}}$ bit 信息，降低不确定性  



#### 物理基础：Landauer 原理与ML能耗  
Landauer 原理（1961）：  
> 擦除 1 bit 信息至少消耗 $kT \ln 2$ 焦耳能量  
> （$k$：玻尔兹曼常数，$T$：环境温度）  

ML 模型的物理本质：麦克斯韦妖的数字版本
- 消费电能（训练过程）→ 生成信息（决策规则）→ 降低系统熵  
- 能耗下限：
 - 若模型生成 $I$ bit 信息 → 理论最小能耗 $E_{\min} = I \cdot kT \ln 2$  
 - 实际能耗 $E_{\text{实际}}$（GPU运算）→ 能效比 $\eta = \frac{E_{\min}}{E_{\text{实际}}}$  
- 商业定价依据：  
   - 模型价值 $V \propto I_{\text{ML}}$（信息量）  
   - 成本 $C \propto E_{\text{实际}}$（能耗）  
   - 定价需满足 $V > C$（如自动驾驶模型因高 $I_{\text{ML}}$ 而高价）  
