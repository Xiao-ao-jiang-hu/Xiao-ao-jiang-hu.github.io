---
title: Discovering state-of-the-art reinforcement learning algorithms
tags:
  - machine-learning
  - reinforcement-learning
categories:
  - paper-reading
  - machine-learning
excerpt: 论文阅读
index_img: /img/reading.jpg
banner_img: /img/reading.jpg
abbrlink: 73f8e17d
date: 2025-10-31 04:47:39
---
<embed src="73f8e17d/Discovering-state-of-the-art-reinforcement.pdf" width="100%" height="600px" type="application/pdf">


### Abs  
本文提出一种纯数据驱动的元学习方法，让机器自主发现强化学习更新规则，而无需人类手工设计。作者用一个“元网络”输出策略与预测的目标分布，驱动大量智能体在复杂环境中并行交互；元网络本身再通过元梯度优化，以最大化长期回报。最终得到的规则 DiscoRL 在 57 款 Atari 游戏上刷新 SOTA（IQM 13.86），并在 ProcGen、Crafter、NetHack 等完全未见的任务上超越或媲美 MuZero、PPO 等主流手工算法。论文首次证明：随着训练环境数量与多样性增加，机器自动发现的更新规则可单调提升泛化性能，实现“AI 自己设计 RL 算法”。

### 1. Intro  
背景  
- 现有 RL 成功算法（DQN、PPO、MuZero）均依赖人类专家多年手工设计，耗时且受限于直觉。  
- 生物进化却在漫长试错中自动产生了适用于动物的高效学习机制。  

开放问题  
1. 手工设计更新规则效率低、难以突破人类认知盲区。  
2. 早期“自动发现 RL”工作搜索空间窄（仅调超参或损失系数），且只在格子世界等简单任务验证，无法与主流算法公平竞争。  

本文贡献  
1. 提出完全自主的元学习框架：用元网络参数化整个更新规则（策略+预测目标），让智能体在大规模复杂环境种群中交互，并通过元梯度优化元网络。  
2. 搜索空间覆盖既有概念（价值、策略、辅助任务）与任意新预测，可重新发现现有算法或创造全新目标。  
3. 发现的 DiscoRL 在 Atari57 上 IQM 13.86 刷新 SOTA，并在 ProcGen、Crafter、NetHack 等未见任务上超越 MuZero、PPO。  
4. 证明随训练环境数量与多样性增加，所得规则通用性与效率单调提升，首次在最具挑战性基准上全面超越人工。

### 2. Background  
| 概念                        | 说明                                                                             |
| --------------------------- | -------------------------------------------------------------------------------- |
| 元学习                      | “学会如何学”，外层慢速优化器调整内层快速学习算法。                               |
| 元梯度                      | 将内层学习过程展开，通过链式法则求外层参数梯度；本文首次用于发现完整更新规则。   |
| Bootstrapping               | 用未来预测构造当前学习目标（TD、Q-learning）；元网络天然支持任意步长 bootstrap。 |
| Atari57 / ProcGen / Crafter | 标准 RL 基准：需长期信用分配、泛化或多技能生存，测样本效率与通用性。             |
| 价值 vs 策略                | 传统 RL 手工指定“预测什么”与“如何更新”；本文把两部分全部交由元网络自动发现。     |

### 3. Method  

#### 3.1 总体思路  
观察到任何 RL 算法核心都是“把预测/策略朝某个目标更新”，而目标无非是对未来奖励/预测的函数。于是：  
- 用通用函数逼近器（元网络）输出这些目标；  
- 让大量智能体在复杂环境中并行交互，形成双层优化：  
  - 内层：智能体按元网络目标更新自身参数；  
  - 外层：元梯度优化元网络，使所有智能体长期回报最大。

#### 3.2 Agent 网络：可发现语义的预测空间  
- 除手工给出的动作价值 $q(s,a)$ 与辅助策略 $p(s,a)$ 外，智能体还输出两组无语义向量：  
  - $y(s)\in\mathbb{R}^n$：仅依赖观测（可表示 V、后继特征等）  
  - $z(s,a)\in\mathbb{R}^m$：同时依赖动作（可表示 Q、动作-条件回报等）  
- 形式覆盖现有概念，但不限于此，留给元网络自由发明新预测。

#### 3.3 元网络：参数化整个更新规则  
- 输入：一段轨迹内智能体输出 $\{\pi,y,z,q\}$、奖励 $r$、终止标志 $b$；用LSTM 反向展开可看任意步未来。  
- 输出：为目标策略 $\hat\pi$、预测 $\hat y,\hat z,\hat q,\hat p$ 生成目标分布。  
- 关键设计  
  - 不直接看像素，只看智能体预测→对观测空间通用；  
  - 动作维度共享权重→任意离散动作数；  
  - 天然支持 bootstrap：未来 $y,z$ 再次输入形成当前目标。  
- 可选增强：meta-RNN 沿“参数更新轴”前向展开，可跟踪生命周期统计量（如回报均值），从而发现奖励归一化等技巧。

#### 3.4 双层优化公式  
- Agent Loss（内层，一条轨迹内采样得到）  
  $$
  L(\theta)=\mathbb{E}_{(s,a)\sim\pi_\theta}\Bigl[D_{\text{KL}}(\hat\pi\|\pi_\theta)+D_{\text{KL}}(\hat y\|y_\theta)+D_{\text{KL}}(\hat z\|z_\theta)+D_{\text{KL}}(\hat q\|q_\theta)+D_{\text{KL}}(\hat p\|p_\theta)\Bigr]
  $$
  只有 $(s,a)$ 采样分布来自环境探索；其余目标均由元网络计算。

  Eg. 在 PPO 中，公式里所有带帽子的目标量 $\hat\pi,\hat y,\hat z,\hat q,\hat p$ 都只由当前轨迹的样本和手工代数式计算得出，不含任何可学习参数。它们就是 PPO 为人类设计算法所定义的“更新方向”，对应关系如下：

| 符号      | 在 PPO 中的具体含义 | 如何由轨迹闭式算出                                                                                                    |
| --------- | ------------------- | --------------------------------------------------------------------------------------------------------------------- |
| $\hat\pi$ | 策略目标分布        | 指数加权优势：<br>$\hat\pi(a \mid s)\propto\pi_{\theta_{\text{old}}}(a \mid s)\cdot\exp\!\bigl(A_t/\varepsilon\bigr)$ |
| $\hat q$  | 动作价值目标        | 广义优势返回：<br>$\hat q(s_t,a_t)=A_t+V_{\theta}(s_t)$                                                               |
| $\hat y$  | 观测-条件预测目标   | 在 PPO 中未显式定义，可退化为 0 或恒向量 → 对应空预测                                                                 |
| $\hat z$  | 动作-条件预测目标   | 同样未显式定义，可退化为 0 或动作独热 → 空预测                                                                        |
| $\hat p$  | 辅助策略目标        | 通常设为 下一状态策略 $\pi_{\theta}(s_{t+1})$（用于一致性正则）                                                       |

因此，在纯 PPO 场景下：  
- $\hat\pi$ 与 $\hat q$ 是真正驱动更新的目标；  
- $\hat y,\hat z$ 为空（或常数），表示“不额外学习新预测”；  
- $\hat p$ 只是辅助正则项，保持动作预测一致性。

它们全部只依赖同一条轨迹内的样本和手工公式，权重固定，可以视为“静态元网络”。

- Meta Objective（外层）  
  $$
  J(\eta)=\mathbb{E}_{\mathcal{E}}\Bigl[\mathbb{E}_\theta\bigl[\textstyle\sum\gamma^t r_t\bigr]\Bigr],\quad
  \nabla_\eta J\approx \mathbb{E}_{\mathcal{E}}\Bigl[\nabla_\eta\theta\cdot\nabla_\theta J(\theta)\Bigr]
  $$
  回传 20 步 Agent 更新（滑动窗口），用 A2C 估计优势；群体梯度经 Adam 预处理后平均，并加入熵正则与 KL 平滑防止崩溃。

#### 3.5 规模与实现  
- Disco57：128 智能体循环 57 款 Atari，1024 TPUv3×64 h，≈6 亿步/游戏。  
- Disco103：加入 ProcGen & DMLab-30，共 206 智能体，2048 TPUv3×60 h。  
- JAX 全分布式，90 % 回放数据提升样本效率；代码与 Disco103 元参数已开源。



### 4. Evaluation  
| 基准               | 设置                       | 主要对手                              | 结果                                              |
| ------------------ | -------------------------- | ------------------------------------- | ------------------------------------------------- |
| Atari57            | 200 M 帧，网络大小=MuZero  | MuZero, Dreamer, Muesli, PPO, Rainbow | Disco57 IQM 13.86 刷新 SOTA，Wall-clock 省 40 %。 |
| ProcGen 16 游戏    | 500 lvl 训练/1000 lvl 测试 | PPO, MuZero, IMPALA                   | Disco57 平均人类归一化 0.84，全面领先。           |
| Crafter            | 1 M / 20 M 步              | Dreamer, Rainbow, PPO                 | Disco103 1 M 步即人类水平（>50 % 成功率）。       |
| NetHack NeurIPS’21 | 无领域知识                 | 40+ 队 top-4                          | Disco57 平均第 3，仅次专用手工规则。              |
| Sokoban            | 未在训练集                 | MuZero                                | Disco103 接近 MuZero SOTA。                       |

消融与规模  
- 去掉 $y,z$ → Atari IQM 降 35 %；去掉 bootstrap → 降 50 %；仅用 57 个格子世界 → 降 70 %，验证必须用复杂环境。  
- 最佳规则 3 个生命周期（≈6 亿步/游戏）即出现；训练环境从 10→57→103，ProcGen 性能单调提升，展现良好规模效应。


### 5. Conclusion  
核心创新  
1. 把“发现整个 RL 更新规则”抽象为双层元优化，用元网络参数化“预测/策略目标”，搜索空间与表达力量级提升。  
2. 首次在大规模复杂环境种群上跑通元梯度，证明所得规则通用性随数据与计算自然扩展，彻底摆脱简单任务局限。

意义  
为“AI 自己设计学习算法”提供可扩展范式：继续增加环境与算力，有望自动涌现更高级、人类尚未想到的 RL 机制，降低人工设计成本，面向新领域（科学计算、多模态、具身智能）只需提供任务分布即可自动生成高效算法。